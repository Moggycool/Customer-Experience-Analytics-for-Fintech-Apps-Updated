{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ca830add",
            "metadata": {},
            "source": [
                "# Task 1 — Google Play Review Scrape & Clean\n",
                "\n",
                "This notebook demonstrates **Task 1 (Data Collection & Preprocessing)** for the Ethiopian bank reviews project.\n",
                "\n",
                "**Outputs**\n",
                "- `data/raw/reviews_raw.csv` (raw scrape; may include duplicates / missing)\n",
                "- `data/processed/reviews_task1_clean.csv` (final Task 1 deliverable; exactly 5 columns)\n",
                "\n",
                "**Task 1 KPI checks**\n",
                "- Total cleaned reviews: **≥ 1,200**\n",
                "- Cleaned reviews per bank: **≥ 400**\n",
                "- Missing critical fields in cleaned dataset: **0%** (dropped during cleaning)\n",
                "- Date format: `YYYY-MM-DD`\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a30ef539",
            "metadata": {},
            "source": [
                "## 0) Setup\n",
                "We resolve the project root relative to this notebook and initialize standard paths and app configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "998fdb9a",
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import annotations\n",
                "\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "\n",
                "# --- Resolve project root (repo root) ---\n",
                "# notebooks/01_task1_scrape_and_clean.ipynb -> project root assumed to be parent of notebooks/\n",
                "PROJECT_ROOT = Path.cwd()\n",
                "# If running from within notebooks/ directory, go one level up\n",
                "if PROJECT_ROOT.name == \"notebooks\":\n",
                "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
                "\n",
                "print(\"PROJECT_ROOT:\", PROJECT_ROOT.resolve())\n",
                "\n",
                "# --- Ensure `src/` is on sys.path so `import bank_reviews` works ---\n",
                "SRC_DIR = PROJECT_ROOT / \"src\"\n",
                "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
                "    sys.path.insert(0, str(SRC_DIR))\n",
                "\n",
                "# --- Import project config ---\n",
                "from bank_reviews.config import Paths, default_app_config\n",
                "\n",
                "# --- Import project utils (Task 1: use io.py and dates.py explicitly) ---\n",
                "from bank_reviews.utils.io import ensure_parent_dir, read_csv, write_csv\n",
                "from bank_reviews.utils.dates import normalize_date\n",
                "\n",
                "paths = Paths.from_root(PROJECT_ROOT)\n",
                "app_cfg = default_app_config()\n",
                "\n",
                "RAW_CSV = paths.raw_dir / \"reviews_raw.csv\"\n",
                "CLEAN_CSV = paths.processed_dir / \"reviews_task1_clean.csv\"\n",
                "\n",
                "# Ensure output folders exist (via utils.io)\n",
                "ensure_parent_dir(RAW_CSV)\n",
                "ensure_parent_dir(CLEAN_CSV)\n",
                "\n",
                "print(\"RAW_CSV:\", RAW_CSV)\n",
                "print(\"CLEAN_CSV:\", CLEAN_CSV)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "91738bce",
            "metadata": {},
            "source": [
                "## 1) Scrape Google Play reviews\n",
                "This step pulls reviews for each bank app using the `scraping.play_store` module.\n",
                "\n",
                "If you have already scraped and saved raw reviews, you can skip scraping and load `data/raw/reviews_raw.csv` instead."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "221514b1",
            "metadata": {},
            "outputs": [],
            "source": [
                "SCRAPE = True  # set False to reuse existing RAW_CSV\n",
                "N_TARGET_PER_BANK = 450  # scrape above 400 to survive cleaning\n",
                "\n",
                "if SCRAPE:\n",
                "    try:\n",
                "        import importlib\n",
                "        import bank_reviews.scraping.play_store as play_store\n",
                "        importlib.reload(play_store)\n",
                "    except ModuleNotFoundError as e:\n",
                "        raise ModuleNotFoundError(\n",
                "            \"Scraper module not found. Implement src/bank_reviews/scraping/play_store.py first.\"\n",
                "        ) from e\n",
                "\n",
                "    df_raw = play_store.scrape_all_banks(\n",
                "        app_cfg=app_cfg,\n",
                "        n_target_per_bank=N_TARGET_PER_BANK,\n",
                "        sort=\"NEWEST\",\n",
                "    )\n",
                "    if df_raw is None:\n",
                "        raise RuntimeError(\"Scraper returned None; ensure scrape_all_banks returns a pandas DataFrame\")\n",
                "\n",
                "    # Persist raw scrape (via utils.io)\n",
                "    write_csv(df_raw, RAW_CSV)\n",
                "else:\n",
                "    if not RAW_CSV.exists():\n",
                "        raise FileNotFoundError(f\"RAW_CSV not found: {RAW_CSV}. Set SCRAPE=True to create it.\")\n",
                "    df_raw = read_csv(RAW_CSV)\n",
                "\n",
                "print(\"Raw shape:\", df_raw.shape)\n",
                "df_raw.head(3)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73d9e44d",
            "metadata": {},
            "source": [
                "### 1.1) Quick raw sanity checks\n",
                "Raw data may include duplicates and missing fields. We check bank distribution and missingness before cleaning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7775005",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count by bank (raw)\n",
                "if \"bank\" in df_raw.columns:\n",
                "    display(df_raw[\"bank\"].value_counts(dropna=False))\n",
                "else:\n",
                "    print(\"WARNING: raw data has no bank column. Ensure scraper adds bank.\")\n",
                "\n",
                "# Missingness in key fields (raw)\n",
                "key_cols = [c for c in [\"review\", \"rating\", \"date\", \"bank\", \"source\", \"review_id\"] if c in df_raw.columns]\n",
                "missing = df_raw[key_cols].isna().mean().sort_values(ascending=False) if key_cols else None\n",
                "print(\"Missing rate (raw):\")\n",
                "display(missing)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4064fa42",
            "metadata": {},
            "source": [
                "### 1.2) Date parsing audit (explicit dates.py usage)\n",
                "\n",
                "In production, `clean_reviews_task1()` should normalize dates internally via `bank_reviews.utils.dates.normalize_date`.\n",
                "This cell is just an **audit** to confirm raw dates can be normalized cleanly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fe10feb2",
            "metadata": {},
            "outputs": [],
            "source": [
                "if \"date\" in df_raw.columns:\n",
                "    sample = df_raw[\"date\"].dropna().astype(object).head(10).tolist()\n",
                "    normalized = [normalize_date(x) for x in sample]\n",
                "    audit_df = pd.DataFrame({\"raw_date_sample\": sample, \"normalize_date(raw)\": normalized})\n",
                "    display(audit_df)\n",
                "else:\n",
                "    print(\"No 'date' column found in df_raw.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "47c644ce",
            "metadata": {},
            "source": [
                "## 2) Clean to Task 1 deliverable format\n",
                "Cleaning enforces the Task 1 schema: **`review, rating, date, bank, source`** with dates normalized to `YYYY-MM-DD` and duplicates removed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e32b90b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from bank_reviews.preprocessing.clean_reviews import clean_reviews_task1\n",
                "except ModuleNotFoundError as e:\n",
                "    raise ModuleNotFoundError(\n",
                "        \"Cleaner module not found. Implement src/bank_reviews/preprocessing/clean_reviews.py first.\"\n",
                "    ) from e\n",
                "\n",
                "df_clean = clean_reviews_task1(df_raw, dedup_strategy=\"review_id\")\n",
                "\n",
                "print(\"Clean shape:\", df_clean.shape)\n",
                "df_clean.head(5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8ab41b6e",
            "metadata": {},
            "source": [
                "## 3) KPI checks\n",
                "We enforce the Task 1 acceptance criteria."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1415687c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.1) Schema check\n",
                "expected_cols = [\"review\", \"rating\", \"date\", \"bank\", \"source\"]\n",
                "assert list(df_clean.columns) == expected_cols, f\"Expected columns {expected_cols}, got {list(df_clean.columns)}\"\n",
                "\n",
                "# 3.2) Missingness check (should be 0 after dropping)\n",
                "missing_clean = df_clean.isna().mean()\n",
                "print(\"Missing rate (clean):\")\n",
                "display(missing_clean)\n",
                "assert (missing_clean == 0).all(), \"Clean dataset still contains missing values.\"\n",
                "\n",
                "# 3.3) KPI: totals\n",
                "total_n = len(df_clean)\n",
                "per_bank = df_clean[\"bank\"].value_counts()\n",
                "print(\"Total cleaned reviews:\", total_n)\n",
                "display(per_bank)\n",
                "\n",
                "assert total_n >= 1200, f\"KPI failed: total cleaned reviews {total_n} < 1200\"\n",
                "assert (per_bank >= 400).all(), f\"KPI failed: some bank has < 400 reviews: {per_bank.to_dict()}\"\n",
                "\n",
                "# 3.4) Date format check\n",
                "date_ok = df_clean[\"date\"].astype(str).str.match(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
                "bad_dates = df_clean.loc[~date_ok, \"date\"].head(10).tolist()\n",
                "assert date_ok.all(), f\"Date format check failed. Example bad dates: {bad_dates}\"\n",
                "\n",
                "print(\"All Task 1 KPI checks passed.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4ddecc8c",
            "metadata": {},
            "source": [
                "## 4) Save Task 1 deliverable\n",
                "We save the cleaned dataset to `data/processed/reviews_task1_clean.csv`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47e2395b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the cleaned dataframe via utils.io\n",
                "write_csv(df_clean, CLEAN_CSV)\n",
                "print(\"Saved:\", CLEAN_CSV)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34054c26",
            "metadata": {},
            "source": [
                "## 5) Quick inspection\n",
                "A quick look at rating distribution and date range."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e552d45",
            "metadata": {},
            "outputs": [],
            "source": [
                "display(df_clean[\"rating\"].value_counts().sort_index())\n",
                "print(\"Date min/max:\", df_clean[\"date\"].min(), df_clean[\"date\"].max())\n",
                "\n",
                "# Optional: show a few samples per bank\n",
                "display(df_clean.groupby(\"bank\").head(2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c53f88c1",
            "metadata": {},
            "source": [
                "---\n",
                "### Notes\n",
                "- If scraping fails due to network/quotas, re-run with `SCRAPE=False` after a successful scrape has been cached.\n",
                "- For reproducibility, keep raw CSVs (even if gitignored) and document scrape time in your report.\n",
                "- `utils.dates.normalize_date` is audited explicitly above; the cleaner should also use it internally."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "MMenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
